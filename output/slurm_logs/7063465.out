## SLURM PROLOG ###############################################################
##    Job ID : 7063465
##  Job Name : slurm_gpu.sh
##  Nodelist : gpu2003
##      CPUs : 
##  Mem/Node : 31744 MB
## Directory : /oscar/scratch/aghandik/FourierImageDiffuse
##   Job Started : Fri Dec  6 07:32:00 PM EST 2024
###############################################################################
Running on GPUs 0,
LatentDiffusionFrequency: Running in eps-prediction mode
DiffusionWrapper has 274.06 M params.
Keeping EMAs of 370.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 3, 64, 64) = 12288 dimensions.
making attention of type 'vanilla' with 512 in_channels
Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys
Training LatentDiffusionFrequency as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2024-12-06T19-32-11_ffhq-ldm-vq-4/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
#### Data #####
train, FFHQTrain, 20000
validation, FFHQValidation, 2000
accumulate_grad_batches = 1
++++ NOT USING LR SCALING ++++
Setting learning rate to 4.00e-06
Project config
model:
  base_learning_rate: 4.0e-06
  target: ldm.models.diffusion.ddpm_frequency.LatentDiffusionFrequency
  params:
    linear_start: 0.0015
    linear_end: 0.0195
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: image
    image_size: 64
    channels: 3
    monitor: val/loss_simple_ema
    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: 64
        in_channels: 3
        out_channels: 3
        model_channels: 224
        attention_resolutions:
        - 8
        - 4
        - 2
        num_res_blocks: 2
        channel_mult:
        - 1
        - 2
        - 3
        - 4
        num_head_channels: 32
    first_stage_config:
      target: ldm.models.autoencoder.VQModelInterface
      params:
        embed_dim: 3
        n_embed: 8192
        ckpt_path: models/first_stage_models/vq-f4/model.ckpt
        ddconfig:
          double_z: false
          z_channels: 3
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity
    cond_stage_config: __is_unconditional__
data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 21
    num_workers: 5
    wrap: false
    train:
      target: taming.data.faceshq.FFHQTrain
      params:
        size: 256
    validation:
      target: taming.data.faceshq.FFHQValidation
      params:
        size: 256

Lightning config
callbacks:
  image_logger:
    target: main.ImageLogger
    params:
      batch_frequency: 5000
      max_images: 8
      increase_log_steps: false
trainer:
  benchmark: true
  accelerator: ddp
  gpus: 0,

Validation sanity check: 0it [00:00, ?it/s]Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]Summoning checkpoint.

