2024-12-06 19:32:07.208834: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-06 19:32:07.337877: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-12-06 19:32:07.341834: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/ffmpeg-7.0-xny2fb2nz3demdcjhj6x74mcxz4iwmys/lib:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/libiconv-1.17-jwjcds2nmpz7gpstqq22vty7jxldvpec/lib:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/cuda-12.3.0-r72aozfqcf6mc3jurx6ndsw6kbcvmd2d/lib64:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/glew-2.2.0-plawm2jod5t5dzga2ktzstlqjp4fjt4q/lib64:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/boost-1.80.0-harukoycghoq6exyzhd3jonvup3gpqeo/lib:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/zstd-1.5.5-zokfqscxl2bxqzikz5ftrnx6ugyaw4m5/lib:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/mesa-22.1.6-6dbg5gqcfhgvls2kyds4s5qk4xgrltkw/lib:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/zlib-1.2.13-jv5y5e7lstxpe7urzow6wr42b44vclu3/lib:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/expat-2.5.0-zujcztprogy4dznn7eu65xext6qeih4r/lib:/usr/lib/nvidia
2024-12-06 19:32:07.341854: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2024-12-06 19:32:08.866760: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/ffmpeg-7.0-xny2fb2nz3demdcjhj6x74mcxz4iwmys/lib:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/libiconv-1.17-jwjcds2nmpz7gpstqq22vty7jxldvpec/lib:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/cuda-12.3.0-r72aozfqcf6mc3jurx6ndsw6kbcvmd2d/lib64:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/glew-2.2.0-plawm2jod5t5dzga2ktzstlqjp4fjt4q/lib64:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/boost-1.80.0-harukoycghoq6exyzhd3jonvup3gpqeo/lib:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/zstd-1.5.5-zokfqscxl2bxqzikz5ftrnx6ugyaw4m5/lib:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/mesa-22.1.6-6dbg5gqcfhgvls2kyds4s5qk4xgrltkw/lib:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/zlib-1.2.13-jv5y5e7lstxpe7urzow6wr42b44vclu3/lib:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/expat-2.5.0-zujcztprogy4dznn7eu65xext6qeih4r/lib:/usr/lib/nvidia
2024-12-06 19:32:08.866860: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/ffmpeg-7.0-xny2fb2nz3demdcjhj6x74mcxz4iwmys/lib:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/libiconv-1.17-jwjcds2nmpz7gpstqq22vty7jxldvpec/lib:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/cuda-12.3.0-r72aozfqcf6mc3jurx6ndsw6kbcvmd2d/lib64:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/glew-2.2.0-plawm2jod5t5dzga2ktzstlqjp4fjt4q/lib64:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/boost-1.80.0-harukoycghoq6exyzhd3jonvup3gpqeo/lib:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/zstd-1.5.5-zokfqscxl2bxqzikz5ftrnx6ugyaw4m5/lib:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/mesa-22.1.6-6dbg5gqcfhgvls2kyds4s5qk4xgrltkw/lib:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/zlib-1.2.13-jv5y5e7lstxpe7urzow6wr42b44vclu3/lib:/oscar/rt/9.2/software/0.20-generic/0.20.1/opt/spack/linux-rhel9-x86_64_v3/gcc-11.3.1/expat-2.5.0-zujcztprogy4dznn7eu65xext6qeih4r/lib:/usr/lib/nvidia
2024-12-06 19:32:08.866876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Global seed set to 23
/oscar/scratch/aghandik/FourierImageDiffuse/ldm/models/autoencoder.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Global seed set to 23
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.
  rank_zero_deprecation(
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Set SLURM handle signals.

  | Name              | Type             | Params
-------------------------------------------------------
0 | model             | DiffusionWrapper | 274 M 
1 | model_ema         | LitEma           | 0     
2 | first_stage_model | VQModelInterface | 55.3 M
-------------------------------------------------------
274 M     Trainable params
55.3 M    Non-trainable params
329 M     Total params
1,317.516 Total estimated model params size (MB)
/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[rank0]: Traceback (most recent call last):
[rank0]:   File "main.py", line 719, in <module>
[rank0]:     trainer.fit(model, data)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 553, in fit
[rank0]:     self._run(model)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 918, in _run
[rank0]:     self._dispatch()
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 986, in _dispatch
[rank0]:     self.accelerator.start_training(self)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 92, in start_training
[rank0]:     self.training_type_plugin.start_training(trainer)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 161, in start_training
[rank0]:     self._results = trainer.run_stage()
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 996, in run_stage
[rank0]:     return self._run_train()
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1031, in _run_train
[rank0]:     self._run_sanity_check(self.lightning_module)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1115, in _run_sanity_check
[rank0]:     self._evaluation_loop.run()
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 111, in run
[rank0]:     self.advance(*args, **kwargs)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py", line 110, in advance
[rank0]:     dl_outputs = self.epoch_loop.run(
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 111, in run
[rank0]:     self.advance(*args, **kwargs)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py", line 110, in advance
[rank0]:     output = self.evaluation_step(batch, batch_idx, dataloader_idx)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py", line 154, in evaluation_step
[rank0]:     output = self.trainer.accelerator.validation_step(step_kwargs)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 211, in validation_step
[rank0]:     return self.training_type_plugin.validation_step(*step_kwargs.values())
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 386, in validation_step
[rank0]:     return self.model(*args, **kwargs)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py", line 93, in forward
[rank0]:     output = self.module.validation_step(*inputs, **kwargs)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/oscar/scratch/aghandik/FourierImageDiffuse/ldm/models/diffusion/ddpm.py", line 359, in validation_step
[rank0]:     _, loss_dict_no_ema = self.shared_step(batch)
[rank0]:   File "/oscar/scratch/aghandik/FourierImageDiffuse/ldm/models/diffusion/ddpm_frequency.py", line 538, in shared_step
[rank0]:     loss = self(x, c)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/oscar/scratch/aghandik/FourierImageDiffuse/ldm/models/diffusion/ddpm_frequency.py", line 550, in forward
[rank0]:     return self.p_losses(x, c, t, *args, **kwargs)
[rank0]:   File "/oscar/scratch/aghandik/FourierImageDiffuse/ldm/models/diffusion/ddpm_frequency.py", line 686, in p_losses
[rank0]:     model_output = self.apply_model(x_noisy, t, cond)
[rank0]:   File "/oscar/scratch/aghandik/FourierImageDiffuse/ldm/models/diffusion/ddpm_frequency.py", line 658, in apply_model
[rank0]:     x_recon = self.model(x_noisy, t, **cond)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/oscar/scratch/aghandik/FourierImageDiffuse/ldm/models/diffusion/ddpm.py", line 1406, in forward
[rank0]:     out = self.diffusion_model(x, t)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/users/aghandik/.conda/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/oscar/scratch/aghandik/FourierImageDiffuse/ldm/modules/diffusionmodules/openaimodel.py", line 736, in forward
[rank0]:     h = th.cat([h, hs.pop()], dim=1)
[rank0]: RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 18 but got size 17 for tensor number 1 in the list.
